clc
clear all
close all

load('drug_comsuption.mat')
X = DB(:,1:12);
%Yamphet 14, Ycaffeine 17, Ycanabis 18, Ynicotine 29

disp('1. Amphetamine')
disp('2. Caffeine')
disp('3. Cannabis')
disp('4. Nicotine')
drug = input('Seleccione la droga a analizar: ');
switch drug
    case 1
        Yreal = DB(:,14);
    case 2
        Yreal = DB(:,17);
    case 3
        Yreal = DB(:,18);
    case 4
        Yreal = DB(:,29);
end
Y = change3Class(Yreal);    %%se transforma la salidas de 7 clases a 3

rept = 10; % Aqui establezco el numero de pliegues que se usaran para la validacion cruzada y las iteraciones para entrenar (Deben ser las mismas)
numClases=length(unique(Y)); %%% Se determina el número de clases del problema.
numMuestras=size(X, 1); % Aqui determino cuantas son las muestras de entrenamiento

if(drug == 1) %% Se hace SMOTE en la clase 3 de anfetamina (desbalance)
    [X, Y] = tecnicaSMOTE(X, Y, numClases, numMuestras, 3);
end
if(drug == 2)
    [X, Y] = tecnicaSMOTE(X, Y, numClases, numMuestras, 1);
    [X, Y] = tecnicaSMOTE(X, Y, numClases, numMuestras, 2);
end
disp('1. K Vecinos más cercanos')
disp('2. Ventana de Parzen')
disp('3. RNA')
disp('4. Random Forest')
disp('5. SVM')
disp('6. Análisis variables - Correlación y Fisher')
disp('6. Análisis variables - Correlación y Fisher')
switch input('Ingrese el numeral del modelo a elegir: ')   
    case 1      %%% modelo kNN %%%
        k = 4;
        modeloKNN(X, Y, k);
    case 2      %%% modelo ventana de Parzen %%%
        h = 0.5;
        modeloParzenWindow(X, Y, h);
    case 3      %%% modelo Redes Neuronales %%%
        epocas = 100;
        capas_neuronas = [15];
        modeloRNA(X, Y, epocas, capas_neuronas, numMuestras);
    case 4      %%% modelo de Random forest %%%
        numArboles = 10;
        modeloRandomForest(rept, numClases, numMuestras, numArboles, X, Y);
    case 5      %%% modelo de SVM %%%
        boxConstraint = 0.01;
        gamma = 0.01;
        tipoK = 1;
        modeloSVM(rept, numClases, numMuestras, boxConstraint, gamma, tipoK, X, Y);
    case 6
        alpha = 0.05;
        variableCorrelacionFisher(X, Y, alpha);   
    case 7
         %(SFS)
    
    Rept=10; %Pliegues para la validación cruzada
    NumMuestras=size(X,1); %Muestras de entrenamiento
    EficienciaTest=zeros(1,Rept); %Vector con la eficiencia de los modelos en cada iteración
    
    %Variables para el proceso de seleccion de caracteriticas
    %Si desea ver los resultados en cada iteracion use 'iter', pero si solo desea ver el resultado final use 'final'
    opciones = statset('display','iter');
    %Use 'forward' para busqueda hacia adelante o 'backward' para busqueda hacia atras
    sentido = 'forward'; 
    
    %Selección de caracteristicas:
    %en el primer return estara 0 o 1 si se incluye o no la caracteristica
    %en el segundo return se guarda el historial de las caracteristicas añadidas.
    [caracteristicasElegidas, proceso] = sequentialfs(@funcionForest,X,Y,'direction',sentido,'options',opciones);
    
    %Se dejan las caracteristicas a trabajar, antes del entrenamiento y validación del modelo.
    X = X(:, caracteristicasElegidas);
    
    %Entrenamiento y validacion del sistema
    for fold=1:Rept
        %Se particionan las muestras de entrenamiento y prueba%
        rng('default');
        particion=cvpartition(NumMuestras,'Kfold',Rept);
        indices=particion.training(fold);
        Xtrain=X(particion.training(fold),:);
        Xtest=X(particion.test(fold),:);
        Ytrain=Y(particion.training(fold));
        Ytest=Y(particion.test(fold));  
        
        %Normalización de datos%
        [Xtrain,mu,sigma] = zscore(Xtrain);
        Xtest = (Xtest - repmat(mu,size(Xtest,1),1))./repmat(sigma,size(Xtest,1),1);
        
        % Estas instrucciones lo que hacen es que realizan la seleccion de caracteristicas en cada iteracion. Pero esto es muy demorado!!
        
% % % % %         [caracteristicasElegidas, proceso] = sequentialfs(@funcionForest,Xtrain,Ytrain,'direction',sentido,'options',opciones);
% % % % %         XReducidas = X(:, caracteristicasElegidas);
% % % % %         [NumMuestras,~] = size(XReducidas);
% % % % %         indices=randperm(NumMuestras);
% % % % %         porcionEntrenamiento = round(NumMuestras*0.7);
% % % % %         Xtrain=XReducidas (indices(1:porcionEntrenamiento),:);
% % % % %         Xtest=XReducidas(indices(porcionEntrenamiento+1:end),:);
% % % % %         Ytrain=Y(indices(1:porcionEntrenamiento),:);
% % % % %         Ytest=Y(indices(porcionEntrenamiento+1:end),:);
        
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        
        %Entrenamiento del modelo
        NumArboles=10;
        Modelo = TreeBagger(NumArboles,Xtrain,Ytrain);
        
        %Se hayan las predicciones del modelo según el modelo
        %entrenado y las muestras separadas para la validacion
        Yest = predict(Modelo,Xtest);
        Yest = str2double(Yest);
        
        %Y se calcula la eficiencia de la iteracion
        EficienciaTest(fold) = sum(Ytest == Yest)/length(Ytest);
    end
    
    %Se muestra cuál fue la eficiencia promedio del sistema
    Eficiencia = mean(EficienciaTest);
    IC = std(EficienciaTest);
    Texto=['La eficiencia obtenida fue = ', num2str(Eficiencia),' +- ',num2str(IC)];
    disp(Texto);
    otherwise
        disp('Numero inválido')
end